# Load necessary libraries
library(MASS)        # For matrix operations
library(matrixcalc)  # For matrix calculations
library(glmnet)      # For lasso penalization
library(FactoMineR)  # For PCA
library(quadprog)    # For quadratic programming
library(forecast)    # For ARIMA comparison
library(zoo)         # For rolling operations
library(ggplot2)     # For visualization
# Example with simulated data
set.seed(123)
T <- 200  # Number of time periods
p <- 50   # Number of assets
returns <- matrix(rnorm(T * p, mean = 0.001, sd = 0.02), ncol = p)
# Ensure data is properly scaled (optional)
returns <- scale(returns)
################################################################################
# Kernel function (Epanechnikov)
epanechnikov_kernel <- function(u) {
ifelse(abs(u) <= 1, 0.75 * (1 - u^2), 0)
}
# Boundary kernel
boundary_kernel <- function(t, r, T, h, kernel_func) {
scaled_diff <- (t - r) / (T*h)  # Argument of the kernel
k_val <- kernel_func(scaled_diff) / h  # Scale by h
# Determine the region of r
Th_floor <- floor(T * h)
if (r < Th_floor) {
# Lower boundary case
integral_val <- integrate(kernel_func, lower = -r / (T * h), upper = 1)$value # articles are conflicting, upper = 1 vs. Inf
return(k_val / integral_val)
} else if (r > (T - Th_floor)) {
# Upper boundary case
integral_val <- integrate(kernel_func, lower = -1, upper = (1 - r / T) / h)$value # articles are conflicting, lower = -1 vs. -Inf
return(k_val / integral_val)
} else {
# Middle region
return(k_val)
}
}
# Two-fold convolution kernel
two_fold_convolution_kernel <- function(u, kernel_func) {
result <- ifelse(
abs(u) <= 2, # Not entirely sure about this, maybe it is dependent on kernel_func?
sapply(u, function(u_val) {
integrand <- function(v) kernel_func(v) * kernel_func(u_val - v)
integrate(integrand, lower = -1, upper = 1)$value
}),
0
)
return(result)
}
# Function to Compute V_m (Sum of Squared Residuals)
compute_V_m <- function(returns, m, kernel_func, bandwidth) {
p <- ncol(returns)
T <- nrow(returns)
total_ssr <- 0  # Sum of squared residuals
for (x in 1:T) {
# Compute boundary kernel weights for time x
w_x <- sapply(1:T, function(t) boundary_kernel(x, t, T, bandwidth, kernel_func))
w_x <- w_x / sum(w_x)  # Normalize weights
# Apply weights to returns
sqrt_w_x <- sqrt(w_x)
weighted_returns <- sweep(returns, 1, sqrt_w_x, `*`)
# Perform PCA
pca_result <- prcomp(weighted_returns, center = FALSE, scale. = FALSE)
num_pcs <- min(m, ncol(pca_result$x))
if (num_pcs < 1) next  # Skip if no PCs are found
# Extract factor scores and loadings
Fhat <- pca_result$x[, 1:num_pcs, drop = FALSE] / sqrt(T)  # Normalize
loadings_hat <- pca_result$rotation[, 1:num_pcs, drop = FALSE]
# Compute fitted values and residuals
fitted <- Fhat %*% t(loadings_hat)
Resid_x <- returns - fitted
total_ssr <- total_ssr + sum(Resid_x^2)
}
# Compute V_m
V_m <- total_ssr / (p * T)
return(V_m)
}
# Function to Select Optimal Number of Factors using Information Criterion
select_optimal_factors <- function(returns, max_factors, T_h, kernel_func, bandwidth) {
p <- ncol(returns)
T <- nrow(returns)
IC_values <- numeric(max_factors)
V_m_values <- numeric(max_factors)
penalty_values <- numeric(max_factors)
for (m in 1:max_factors) {
V_m <- compute_V_m(returns, m, kernel_func, bandwidth)
V_m_values[m] <- V_m
# Compute penalty
penalty <- (p + T_h) / (p * T_h) * log((p * T_h) / (p + T_h)) * m
penalty_values[m] <- penalty
# Compute Information Criterion (IC)
IC_values[m] <- log(V_m) + penalty
}
optimal_m <- which.min(IC_values)
return(list(optimal_m = optimal_m, IC_values = IC_values, V_m_values = V_m_values, penalty_values = penalty_values))
}
# Function to Perform Local PCA
local_pca <- function(returns, r, bandwidth, m, kernel_func) {
T <- nrow(returns)
# Compute boundary kernel weights for time r
w_r <- sapply(1:T, function(t) boundary_kernel(r, t, T, bandwidth, kernel_func))
w_r <- w_r / sum(w_r)  # Normalize weights
# Apply weights to returns
sqrt_w_r <- sqrt(w_r)
weighted_returns <- sweep(returns, 1, sqrt_w_r, `*`)
# Perform PCA
pca_result <- prcomp(weighted_returns, center = FALSE, scale. = FALSE)
# Determine actual number of factors
num_factors <- min(m, ncol(pca_result$x))
if (num_factors < 1) return(NULL)  # Return NULL if no factors are found
# Extract factor scores and loadings
Fhat_all <- pca_result$x[, 1:num_factors, drop = FALSE] / sqrt(T)  # Normalize
loadings_all <- pca_result$rotation[, 1:num_factors, drop = FALSE]
return(list(factors_full = Fhat_all, loadings_full = loadings_all))
}
# Apply local PCA over all time points
bandwidth <- (2.35/sqrt(12))*T^(-0.2)*p^(-0.1) # Silverman's rule of thumb
m <- select_optimal_factors(returns, max_factors = 10, T_h = T*bandwidth, kernel_func = epanechnikov_kernel, bandwidth)$optimal_m # Number of factors
factors_list <- list()
loadings_list <- list()
for (t in 1:T) {
pca_result <- local_pca(returns, t, bandwidth, m, epanechnikov_kernel)
factors_list[[t]] <- pca_result$factors
loadings_list[[t]] <- pca_result$loadings
}
compute_M_hat <- function(local_factors, global_factors, local_loadings, global_loadings, T, N, m) {
M_hat <- 0
if (m ==1){
global_loadings <- matrix(global_loadings)
global_factors <- matrix(global_factors)
}
for (i in 1:N) {
for (t in 1:T) {
common_H1 <- t(local_loadings[[t]][i, ]) %*% local_factors[[t]][t,]
common_H0 <- (global_loadings[i, ]) %*% global_factors[t, ]
M_hat <- M_hat + (common_H1 - common_H0)^2
}
}
M_hat <- M_hat / (N * T)
return(M_hat)
}
compute_B_pT <- function(local_factors, global_factors, residuals, h, T, p, kernel_func) {
# 1) Precompute sum of residual squares per row s
res2 <- rowSums(residuals^2)  # length T
# 2) Kernel matrix K[s,t]
K <- matrix(0, nrow=T, ncol=T)
for (s in 1:T) {
for (t in 1:T) {
K[s, t] <- boundary_kernel(s, t, T, h, kernel_func)
}
}
# 3) Local dot-product matrix, L[s,t] = l_s . l_t
#    where l_s = local_factors[[s]][s, ], a length-r vector.
L <- matrix(0, nrow=T, ncol=T)
for (s in 1:T) {
ls <- local_factors[[s]][s, ]
for (t in 1:T) {
lt <- local_factors[[t]][t, ]
L[s, t] <- sum(ls * lt)
}
}
# 4) Global dot-product matrix, G[s,t] = g_s . g_t
#    if global_factors is T x r
G <- global_factors %*% t(global_factors)
# 5) Vectorized calculation of (K*L - G)^2, then multiply row s by res2[s], sum over all s,t
D <- (K * L) - G  # elementwise
D2 <- D^2
val <- sum(D2 * res2[row(D2)])
# 6) Final scaling
B_pT <- (h^(1/2) / (T^2 * sqrt(p))) * val
return(B_pT)
}
compute_V_pT <- function(local_factors, residuals, h, T, p, factor_cov, kernel_func) {
V_pT <- 0
for (s in 1:(T - 1)) {
for (r in (s + 1):T) {
k_bar_sr <- two_fold_convolution_kernel((s - r) / (T * h), kernel_func)
term <- k_bar_sr^2 * (t(local_factors[[s]][s,]) %*% factor_cov %*% local_factors[[r]][r,])^2
V_pT <- V_pT + term * (t(residuals[s, ]) %*% residuals[r, ])^2
}
}
V_pT <- (2 / (T^2 * p * h)) * V_pT
return(V_pT)
}
compute_J_pT <- function(B_pT, V_pT, M_hat, T, p, h) {
J_pT <- (T * sqrt(p) * sqrt(h) * M_hat - B_pT) / sqrt(V_pT)
return(J_pT)
}
# Factor covariance matrix
factor_covariance <- cov(do.call(rbind, factors_list))  # Aggregate over all factors
# Estimate residuals
residuals <- matrix(NA, nrow = T, ncol = p)
for (t in 1:T) {
modeled_returns_t <- factors_list[[t]] %*% t(loadings_list[[t]])
residuals[t, ] <- returns[t, ] - modeled_returns_t[t,]
}
# Function to Estimate Residual Covariance with Lasso Penalization
estimate_residual_cov <- function(residuals, lambda) {
p <- ncol(residuals)
# Compute the sample covariance matrix
S <- cov(residuals)
# Initialize the sparse covariance matrix
sparse_cov <- matrix(0, nrow = p, ncol = p)
# Apply Lasso regression row-wise
for (i in 1:p) {
response <- S[i, -i]
predictors <- S[-i, -i]
# Fit Lasso regression without intercept
fit <- glmnet(predictors, response, alpha = 1, lambda = lambda, intercept = FALSE, standardize = FALSE)
# Extract coefficients and assign to sparse covariance matrix
coef_i <- as.vector(coef(fit, s = lambda))[-1]  # Exclude intercept
sparse_cov[i, -i] <- coef_i
}
# Symmetrize the covariance matrix
sparse_cov <- (sparse_cov + t(sparse_cov)) / 2
# Assign original diagonal elements
diag(sparse_cov) <- diag(S)
return(sparse_cov)
}
lambda <- 0.1  # Set penalty parameter
residual_covariance <- estimate_residual_cov(residuals, lambda)
# Compute Time-Varying Covariance Matrices
compute_time_varying_cov <- function(loadings, factor_cov, residual_cov) {
return(loadings %*% factor_cov %*% t(loadings) + residual_cov)
}
time_varying_cov_list <- mapply(
compute_time_varying_cov,
loadings = loadings_list,
MoreArgs = list(factor_cov = factor_covariance, residual_cov = residual_covariance),
SIMPLIFY = FALSE
)
compute_optimal_weights <- function(cov_matrix, p) {
if (any(is.na(cov_matrix)) || !is.positive.definite(round(cov_matrix, 10))) {
# Add a small ridge term to stabilize inversion
cov_matrix <- cov_matrix + diag(1e-6, p)
}
Dmat <- solve(cov_matrix)
dvec <- rep(0, p)
Amat <- matrix(1, nrow = p, ncol = 1)  # Constraint: sum(weights) = 1
bvec <- 1
# Solve QP
result <- tryCatch(
solve.QP(Dmat, dvec, Amat, bvec, meq = 1),
error = function(e) NULL
)
if (!is.null(result)) {
return(result$solution)
} else {
# Return equal weights if QP fails
return(rep(1/p, p))
}
}
optimal_weights <- lapply(time_varying_cov_list, function(cov_mat) {
compute_optimal_weights(cov_mat, p)
})
compute_portfolio_risk <- function(weights, cov_matrix) {
return(sqrt(as.numeric(t(weights) %*% cov_matrix %*% weights)))
}
portfolio_risk <- mapply(
compute_portfolio_risk,
weights = optimal_weights,
cov_matrix = time_varying_cov_list,
SIMPLIFY = TRUE
)
library(devtools)
remove.packages("TVMVP")
devtools::clean_dll()
build()
install()
# Example
setwd("C:/Users/erikl_xzy542i/Documents/Master_local/Thesis/TV-MVP/TVMVP")
# Load necessary libraries
library(MASS)        # For matrix operations
library(matrixcalc)  # For matrix calculations
library(glmnet)      # For lasso penalization
library(FactoMineR)  # For PCA
library(quadprog)    # For quadratic programming
library(forecast)    # For ARIMA comparison
library(zoo)         # For rolling operations
library(ggplot2)     # For visualization
library(TVMVP)
library(PerformanceAnalytics)
library(devtools)
remove.packages("TVMVP")
devtools::clean_dll()
build()
install()
# Load necessary libraries
library(MASS)        # For matrix operations
library(matrixcalc)  # For matrix calculations
library(glmnet)      # For lasso penalization
library(FactoMineR)  # For PCA
library(quadprog)    # For quadratic programming
library(forecast)    # For ARIMA comparison
library(zoo)         # For rolling operations
library(ggplot2)     # For visualization
library(TVMVP)
library(PerformanceAnalytics)
